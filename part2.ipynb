{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "imports\n",
    "'''\n",
    "\n",
    "#standard utilities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import time\n",
    "\n",
    "#plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, roc_curve, auc\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "from gensim.models import FastText\n",
    "import fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.read_parquet(\"data/TweetsCOV19_preprocessed.parquet\")\n",
    "glove_file = 'data/glove.twitter.27B.25d.txt'  # Path to the GloVe file\n",
    "wv_glove = KeyedVectors.load_word2vec_format(glove_file, binary=False)\n",
    "default_glove = np.zeros(25, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_file = 'data/GoogleNews-vectors-negative300.bin'\n",
    "wv_word2vec = KeyedVectors.load_word2vec_format(word2vec_file, binary=True, limit=2000000)\n",
    "default_word2vec = np.zeros(300, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FastText_file = 'data/cc.en.300.bin'\n",
    "wv_fasttext = fasttext.load_model(FastText_file)\n",
    "wv_fasttext[\"hello\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nVader Q1\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Vader Q1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nVader Q2\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Vader Q2\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50, 47, 46, 46, 44, 44, 44, 44, 43, 43, 43, 42, 42, 41, 41, 41, 41, 41, 40, 40, 39, 39, 39, 39, 39, 39, 39, 39, 38, 38, 38, 38, 38, 38, 38, 38, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 37, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35, 35]\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "dftr = pd.read_parquet(\"data/train.parquet\")\n",
    "dfte = pd.read_parquet(\"data/test.parquet\")\n",
    "dfvl = pd.read_parquet(\"data/valid.parquet\")\n",
    "df = pd.concat([dftr, dfte])\n",
    "\n",
    "max_words = sorted([len(string.split(' ')) for string in df['X'].values], reverse=True)\n",
    "print(max_words[0:100])\n",
    "max_words = max_words[0]\n",
    "#[word for string in df['X'].values for word in string.split(' ')]\n",
    "\n",
    "unique_sentences = list(set([string for string in df['X'].values]))\n",
    "unique_words = list(set([word for string in df['X'].values for word in string.split(' ')]))\n",
    "unique_words_sorted = sorted(unique_words)\n",
    "#print(len(unique_words))\n",
    "#print(unique_words[0:100])\n",
    "#print(' ' in unique_sentences)\n",
    "#print('' in unique_words)\n",
    "#print(all([not string.isspace() for string in unique_words]))\n",
    "#print(np.any(dftr[\"X\"].values == ' '))\n",
    "print(np.any([[word for word in sentence] for sentence in dftr[\"X\"].values] == ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1\n",
    "def bag_of_words(tweets_array):\n",
    "    vectorizer = CountVectorizer(vocabulary=unique_words)\n",
    "    X = vectorizer.fit_transform(tweets_array)\n",
    "    \n",
    "    return X.astype(np.bool_)\n",
    "\n",
    "#bag_of_words(dftr[\"X\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q2\n",
    "def tf_idf(tweets_array):\n",
    "    \n",
    "    tfidf = TfidfVectorizer(vocabulary=np.array(unique_words))\n",
    "\n",
    "    tfidf.fit(tweets_array)\n",
    "    X = tfidf.transform(tweets_array)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q3\n",
    "def word_2_vec_skipgram(tweets_array, agg):\n",
    "    \n",
    "    X = [np.float16(agg([wv_word2vec[word] if word in wv_word2vec else default_word2vec for word in sentence], axis=0)) for sentence in tweets_array]\n",
    "\n",
    "    return X\n",
    "\n",
    "#print(word_2_vec_skipgram(dftr[\"X\"].values));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q4\n",
    "def glove_emb(tweets_array, agg):\n",
    "    #print(np.any([[word for word in sentence] for sentence in tweets_array] == ' '))\n",
    "    X = [np.float16(agg([wv_glove[word] if word in wv_glove else default_glove for word in sentence], axis=0)) for sentence in tweets_array]\n",
    "    return X\n",
    "\n",
    "#print(word_2_vec_skipgram(dftr[\"X\"].values));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q5\n",
    "def fast_text(tweets_array, agg):\n",
    "    X = [np.float16(agg([wv_fasttext[word] if word in wv_fasttext else default_word2vec for word in sentence], axis=0)) for sentence in tweets_array]\n",
    "    return X\n",
    "    \n",
    "\n",
    "#fast_text(dftr[\"X\"].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q7\n",
    "agg1 = np.mean\n",
    "agg2 = np.maximum.reduce\n",
    "agg3 = np.minimum.reduce\n",
    "# max_words = 50\n",
    "# def agg3(l_vec, axis):\n",
    "#     ir = np.concatenate(l_vec)\n",
    "#     #pad with extra zeros if smaller\n",
    "#     print(max_words*l_vec[0].shape[0])\n",
    "#     print(ir.shape[0])\n",
    "#     return np.concatenate((np.array(ir), np.zeros(max_words*l_vec[0].shape[0] - ir.shape[0], dtype=type(ir[0]))))\n",
    "    \n",
    "\n",
    "word2vec_avg = lambda x : word_2_vec_skipgram(x, agg=agg1)\n",
    "word2vec_avg.__name__ = \"word2vec_avg\"\n",
    "word2vec_max = lambda x : word_2_vec_skipgram(x, agg=agg2)\n",
    "word2vec_max.__name__ = \"word2vec_max\"\n",
    "word2vec_min = lambda x : word_2_vec_skipgram(x, agg=agg3)\n",
    "word2vec_min.__name__ = \"word2vec_min\"\n",
    "\n",
    "glove_avg = lambda x : glove_emb(x, agg=agg1)\n",
    "glove_avg.__name__ = \"glove_avg\"\n",
    "glove_max = lambda x : glove_emb(x, agg=agg2)\n",
    "glove_max.__name__ = \"glove_max\"\n",
    "glove_min = lambda x : glove_emb(x, agg=agg3)\n",
    "glove_min.__name__ = \"glove_min\"\n",
    "\n",
    "fasttext_avg = lambda x : fast_text(x, agg=agg1)\n",
    "fasttext_avg.__name__ = \"fasttext_avg\"\n",
    "fasttext_max = lambda x : fast_text(x, agg=agg2)\n",
    "fasttext_max.__name__ = \"fasttext_max\"\n",
    "fasttext_min = lambda x : fast_text(x, agg=agg3)\n",
    "fasttext_min.__name__ = \"fasttext_min\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying embedding fasttext_avg\n"
     ]
    }
   ],
   "source": [
    "#Q8\n",
    "\n",
    "#print(dftr)\n",
    "# dfte = pd.read_parquet(\"data/test.parquet\")\n",
    "# dfvl = pd.read_parquet(\"data/valid.parquet\")\n",
    "#test_embedding_matrix = np.random.randint(0, high=1, size=(dftr.shape[0] + dfte.shape[0], len(unique_words)), dtype=\"int\")\n",
    "seed = 1234\n",
    "embeddings = [bag_of_words, tf_idf, word2vec_avg, word2vec_max, word2vec_min, glove_avg, glove_max, glove_min, fasttext_avg, fasttext_max, fasttext_min]\n",
    "\n",
    "dftr = pd.read_parquet(\"data/train.parquet\")\n",
    "dfte = pd.read_parquet(\"data/test.parquet\")\n",
    "dfvl = pd.read_parquet(\"data/valid.parquet\")\n",
    "\n",
    "\n",
    "#classifiers = [svm.SVC, LogisticRegression, RandomForestClassifier]\n",
    "classifiers = [\"SVM\", \"LogRegr\", \"RandomForest\"]\n",
    "ys = [\"positive\", \"negative\"]\n",
    "X_train_raw = dftr[\"X\"].str.replace(\"\\u00A0\",\"\").values\n",
    "X_test_raw = dfte[\"X\"].str.replace(\"\\u00A0\",\"\").values\n",
    "X_valid_raw = dfvl[\"X\"].str.replace(\"\\u00A0\",\"\").values\n",
    "\n",
    "rows = [e.__name__ + \"_\" + c + \"_\" + y for e, c, y in itertools.product(*[embeddings, classifiers, ys])]\n",
    "cols = [\"f1_1\", \"f1_2\", \"f1_3\", \"f1_4\", \"f1_5\", \"f1_avg\", \"duration\"]\n",
    "df_res = pd.DataFrame(index=rows, columns=cols)\n",
    "#df_res.loc[str(word_2_vec_skipgram), str(svm.SVC) +dd \"_positive_f1\"] = 1.0\n",
    "#print(df_res.loc[str(word_2_vec_skipgram), str(svm.SVC) + \"_positive_f1\"])\n",
    "\n",
    "for e in embeddings[8:]:\n",
    "    print(f\"trying embedding {e.__name__}\")\n",
    "    X_train = e(X_train_raw)\n",
    "    X_test = e(X_test_raw)\n",
    "    X_valid = e(X_valid_raw)\n",
    "    \n",
    "    for c in classifiers:\n",
    "        \n",
    "\n",
    "        for y in ys:\n",
    "            #print(f\"trying embedding {str(e)} with classifier {c} for analysis of {y} Sentiment\")\n",
    "            start_time = time.perf_counter()\n",
    "            match c:\n",
    "                case \"SVM\":\n",
    "                    clf = svm.SVC(random_state=seed, kernel=\"rbf\", gamma='scale', class_weight='balanced', C=5.0)\n",
    "                case \"LogRegr\":\n",
    "                    clf = LogisticRegression(random_state=seed, C=5.0, penalty='l2', solver='saga', max_iter=1000, class_weight='balanced')\n",
    "                case \"RandomForest\":\n",
    "                    clf = RandomForestClassifier(random_state=seed, max_depth=20, min_samples_split=0.2, min_samples_leaf=0.1, max_features='log2', class_weight=\"balanced\")\n",
    "            \n",
    "\n",
    "            y_train = dftr[\"y_\" + y[0:3]].values\n",
    "            #print(dftr[\"y_\" + y[0:3]].unique())\n",
    "            y_test = dfte[\"y_\" + y[0:3]].values\n",
    "            #print(y_test)\n",
    "            #print(f\"starting to fit for {c}\")\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            \n",
    "           \n",
    "            #print('Precision: %.3f' % precision_score(y_test, y_pred, average=None))\n",
    "            #print('Recall: %.3f' % recall_score(y_test, y_pred, average=None))\n",
    "            f1 = f1_score(y_test, y_pred, average=None)\n",
    "            f1_avg = f1_score(y_test, y_pred, average=\"macro\")            \n",
    "            \n",
    "            row_name = e.__name__ + \"_\" + c + \"_\" + y\n",
    "            #print(row_name)\n",
    "            #print(f1.shape)\n",
    "            #print(\"f1 score: \", f1)\n",
    "            df_res.loc[row_name, cols[0:f1.shape[0]]] = f1\n",
    "            df_res.loc[row_name, \"f1_avg\"] = f1_avg\n",
    "            \n",
    "            end_time = time.perf_counter()\n",
    "            df_res.loc[row_name, \"duration\"] = np.round(end_time-start_time, 2)\n",
    "            df_res.to_csv(\"data/q8res.csv\")\n",
    "            #print('Accuracy: %.3f' % accuracy_score(y_test, y_pred, average=None))\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = [[0], [1], [2], [3]]\n",
    "Y = np.array([0, 1, 2, 3]).astype(np.int8)\n",
    "clf = svm.SVC(decision_function_shape='ovo')\n",
    "clf.fit(X, Y)\n",
    "svm.SVC(decision_function_shape='ovo')\n",
    "dec = clf.decision_function([[1]])\n",
    "dec.shape[1] # 4 classes: 4*3/2 = 6\n",
    "clf.decision_function_shape = \"ovr\"\n",
    "dec = clf.decision_function([[1]])\n",
    "dec.shape[1] # 4 classes\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "23fs_mlhc_p2-jtQQEGpx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
